\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=2.5cm,right=2.5cm,marginparwidth=1.75cm]{geometry}
\usepackage{multicol}
\usepackage{parskip}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{float}

\usepackage{natbib}
\bibliographystyle{unsrt}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}

\title{EE267 Final Project Report (Option 1)\\Semantic Segmentation for Autonomous Driving\\across Diverse Weather Conditions}


\author{Hefeifei Jiang, Kunyi Yu}

\begin{document}
\maketitle
\begin{multicols}{2}
\setlength{\parskip}{0.1in}
\setlength{\parindent}{15pt}

\begin{abstract}
Autonomous vehicles (AVs) require reliable perception systems to operate safely in diverse environments. Semantic segmentation, which provides pixel-level scene understanding, is highly sensitive to adverse weather such as rain, fog, and snow. In this project, we investigate the robustness of semantic segmentation models across multiple weather conditions using a CARLA-simulated dataset with ground-truth annotations. We evaluate several models and explore training strategies designed to improve cross-weather generalization. Our results show that incorporating diverse weather conditions during training significantly enhances segmentation performance and robustness in challenging environments. The GitHub repository is available\footnote{\url{https://github.com/Rock3Yu/EE267_Project_SegSenarioAdaption}}.
\end{abstract}

\textbf{Keywords:} Autonomous Driving, Semantic Segmentation, Diverse Weather Conditions

% \newcolumn
\section{Introduction}\label{sec:introduction}
Autonomous vehicles (AVs) are rapidly advancing, promising to reform transportation systems. Perception as the foundation of AVs plays a important role in capturing the surrounding environment accurately, so that planning and decision-making modules can make safe and efficient driving behaviors. Semantic segmentation provides pixel-level understanding of the scene, which is critical for AVs to understand the categories of objects and regions (e.g., drivable areas, sidewalks, pedestrians).

However, the accuracy and robustness of semantic segmentation models can be significantly affected by diverse weather conditions, such as rain, fog, and snow. These corner cases introduce huge challenges for AVs' perception systems as well as following planning and decision-making modules. The performance decline in low-visibility scenarios can lead to higher risks of accidents and mistrust among society when deploying AVs in real-world environments.

In this project, we research the popular and state-of-the-art semantic segmentation models, and evaluate their performance across diverse weather conditions. Besides, we collect different weather perception datasets with ground truth annotations from CARLA simulator \cite{dosovitskiy2017carla}. Based on the evaluation results, we modify and improve popular semantic segmentation models to improve their performance in diverse weather conditions. Finally, we conduct ablation studies to analyze the effectiveness of our proposed improvements.

The core contributions of this project are summarized as follows:
\begin{itemize}
    \item Conduct a comprehensive research on existing semantic segmentation models.
    \item Collect and preprocess a diverse weather perception dataset with ground truth annotations from CARLA simulator.
    \item Evaluate the performance of selected models across diverse weather conditions.
    \item Propose and implement improvements to enhance model robustness.
\end{itemize}

The rest of the report is organized as follows: Section~\ref{sec:related_work} reviews related work in semantic segmentation for autonomous driving under diverse weather conditions. Section~\ref{sec:methodology} details the methodology, including dataset collection, model selection, and proposed improvements. Section~\ref{sec:experiments} presents the experimental results and analysis. Finally, Section~\ref{sec:conclusion} concludes the report and discusses future work.

\section{Related Work}\label{sec:related_work}
\textbf{Semantic segmentation} has been widely studied in the context of autonomous driving, given its critical role in scene understanding. The models can be simply categorized into five categories: decoder-based methods, context information-based methods, GAN-based methods, RNN-based methods, and weak supervision-based methods \cite{hao2020brief}.

Classic models based on decoder structures include U-Net, SegNet, and DeepLab series, where the DeepLabV3+ reaches 89.0\% mIoU on the PASCAL VOC 2012 dataset \cite{everingham2011pascal}. Context information-based methods, such as PSPNet, EncNet and DenseASPP, leverage global context or multi-scale context to enhance segmentation performance, where EncNet achieves 85.9\% mIoU of small objects on the PASCAL VOC 2012 dataset. In GAN-based methods, models like SegAN and MMANet outperform in the problem of small sample size and imbalanced class distribution but the training process is unstable. RNN-based methods, such as ReNet, ReSeg, LSTM-RNN, and Graph-LSTM, capture local sptatial features and their correlations. Lastly, weak supervision-based methods like BoxSup and DEXTR reduce the reliance on time-consuming and labor-intensive data annotation \cite{hao2020brief}.

\textbf{Datasets for all-weather segmentation} are critical for the training and evaluation of many learning-based semantic segmentation models. Conditional-specific datasets provide semantic segmentation data under certain weather conditions, for example, the Foggy Cityscapes dataset \cite{sakaridis2018semantic} simulates foggy conditions based on the Cityscapes dataset, and the Raincouver benchmark \cite{tung2017raincouver} focuses on rainy weather. On the other hand, unified all-weather datasets, such as the ACDC dataset \cite{sakaridis2021acdc} contain real-world images captured under four common adverse weather conditions (fog, nighttime, rain, and snow), and the WildDash2 dataset \cite{zendel2023test} includes dashcam footage from diverse weather, lighting, and camera conditions. The pros of those datasets are that they provide comprehensive coverage of various weather scenarios, however, the cons are the high cost and incomplete senario coverage.

\textbf{Data augmentation} and \textbf{Synthetic weather simulation} are increasingly studied and utilized to generate diverse weather conditions due to the nature of low cost and high flexibility. Simple data augmentation techniques include random blurring, noise addition, and brightness/contrast adjustments. Moreover, GAN-based weather translation methods, such as CycleGAN \cite{zhu2017unpaired}, can successfully translate images from clear weather to adverse weather conditions by learning the underlying distribution of weather effects. Simulation-based approaches are also popular, where CARLA simulator \cite{dosovitskiy2017carla} is one of the most widely-used platforms that can simulate various weather conditions in provided urban environments with ground truth annotations.

\textbf{Domain adaption and Curriculum learning} are two effective techniques to improve the robustness of semantic segmentation models across diverse weather conditions. Adversarial feature alignment methods, like CyCADA \cite{hoffman2018cycada}, guide transfer between domains in an unsupervised manner. Pseudo-labeling self-training methods contain AdaptSegNet \cite{tsai2018learning}. Those domain adaptation methods can realize sim2real transfer as well as weather adaptation, however the performance is still limited and unstable for AV systems. Meanwhile, curriculum learning is a widely-used training manner that gradually increases the complexity of training samples which can help models learn better and generalize well especially in complex scenarios like diverse weather conditions.

\textbf{Weather-specific architectures and sensor fusion} are also proposed to enhance segmentation performance in the context of extreme weather. Feature-level normalization techniques can mitigate the impact of weather-induced variations. Multi-sensor fusion approaches, combining inputs from cameras, LiDAR, radar, or other sensors, can provide a more comprehensive understanding of the environment, reduce the rise of sensor-specific failures under specific weather conditions, and improve overall segmentation accuracy. Methods like PointPainting \cite{vora2020pointpainting} demonstrates a promising result.

To summarize, while significant progress and various techniques have been developed for semantic segmentation including the senarios of autonomous driving under diverse weather conditions, challenges remain in achieving robust, reliable and generalizable performance. The gap between synthetic and real-world data persists. Some methods rely heavily on transformer backbones which are computationally expensive and too slow for real-time AV applications. Moreover, mixed extreme weather conditions are rarely studied. This limiation motivates our project to further explore the robustness of basic semantic segmentation models under diverse weather conditions by using a CARLA-simulated dataset with ground-truth annotations, trying to inspire future research in this important area.

\section{Methodology}\label{sec:methodology}
This section describes the overall methodology of our project, including dataset collection and preprocessing, model selection, training strategies, and evaluation metrics. The proposed training strategies ``Mixed-Weather Training'' and ``Weather-Aware Curriculum Learning'' is one of our core contributions.

\subsection{Dataset Collection and Preprocessing}
We utilize the CARLA simulator version 0.9.15 to collect a diverse weather perception dataset with ground truth annotations. The simulator uses the provided docker image from EE267 lab0. After launching the CARLA server, we run a Python script to spawn an ego vehicle and other random vehicles, pedestrians with autopilot mode enabled. The camera sensor is set up at a height of 1.6 meters from the ground, with a field of view (FOV) of 90 degrees and a resolution of 512x512 pixels.

We define several weather conditions and time of day settings in CARLA. The weather conditions include \textit{Clear}, \textit{SoftRain}, and \textit{HardRain}, while the time of day settings include \textit{Noon}, \textit{Sunset}, and \textit{Night}, resulting in a total of 9 different weather-time combinations. A demostation of different weather conditions is shown in Figure~\ref{fig:weather_conditions}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.\linewidth,height=4cm,keepaspectratio]{pics/weather_time.png}
    \caption{Dataset settings with different weather and time combinations.}
    \label{fig:weather_conditions}
\end{figure}

The data annotation classes are defined based on the CARLA semantic segmentation labels, where we choose 19 classes (CityScapes Standard): \textit{Road}, \textit{Vehicle}, \textit{Pedestrian}, \textit{Building}, \textit{Sidewalk}, \textit{Vegetation}, etc.

As for data collection, we run the simulation for each weather-time combination, capturing images and their corresponding semantic segmentation masks at a frequency of 1 frame per 10 seconds to 20 seconds, depending on whether the scene is dynamic enough. We collect a total of 3,600 images where each weather-time combination has around 400 images. The dataset is then split into training and testing sets with an 80-20 ratio.

\subsection{Model Selection}
We select several popular and representative semantic segmentation models for evaluation and further improvement, including: (1) DeepLab V3, (2) FCN, and (3) Lite R-ASPP. These models are widedly used in the studies and as benchmarks for semantic segmentation tasks and their parameters are all pre-trained on a 20-class subset of the COCO dataset.

DeepLab V3 \cite{chen2017rethinking} utilizes atrous convolution and spatial pyramid pooling to capture multi-scale context information effectively. FCN \cite{long2015fully} is one of the pioneering models that introduced the concept of fully convolutional networks for semantic segmentation, enabling end-to-end training and inference. Lite R-ASPP \cite{mehta2019espnetv2} is a lightweight model designed for real-time applications, employing a reduced Atrous Spatial Pyramid Pooling (ASPP) module to balance accuracy and efficiency.

At the time of writing, we mainly focus on DeepLab V3 for performance evaluation and other model implementations are in our future work plan. The structures overview of DeepLab V3 is shown in Figure~\ref{fig:deeplabv3_structure}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.\linewidth,height=6cm,keepaspectratio]{pics/deeplabv3_structure.png}
    \caption{DeepLab V3 structure overview \cite{chen2017rethinking}.}
    \label{fig:deeplabv3_structure}
\end{figure}

\subsection{Training Strategies}
In the training process, we evaluate three different training strategies: 
(0) Clear-Only Training, 
(1) Mixed-Weather Training, and 
(2) Weather-Aware Curriculum Learning. 
These settings are designed to examine how different levels of weather diversity in training data affect model robustness and generalization.

\paragraph{Clear-Only Training (Setting 0).}
In this baseline setting, the model is trained exclusively on clear-weather images and evaluated across all weather conditions. 
This configuration measures the inherent domain gap caused by weather variations, as the model receives no exposure to rain or nighttime conditions during training. 
Although this strategy is computationally efficient and widely used in autonomous-driving perception benchmarks, it typically results in severe performance degradation when the model is deployed outside clear-weather environments.

\paragraph{Mixed-Weather Training (Setting 1).}
In this strategy, training images from all available weather conditions (clear, rain, and night) are randomly mixed within each batch. 
This simple multi-domain training approach is commonly adopted for improving robustness and serves as a strong reference point. 
By exposing the model to diverse weather conditions during training, mixed-weather learning mitigates clear-weather bias; however, it does not explicitly address the imbalance among weather domains and may underfit rare or difficult conditions.

\paragraph{Weather-Aware Curriculum Learning (Setting 2).}
The third strategy adopts a progressive curriculum that gradually increases weather difficulty while balancing the sampling ratios across conditions. 
Training proceeds in three stages:

\begin{itemize}
    \item \textbf{Stage 1:} Clear weather only.
    \item \textbf{Stage 2:} Clear + SoftRain with a 50:50 sampling ratio.
    \item \textbf{Stage 3:} Clear + SoftRain + HardRain with a sampling ratio of 10:20:70.
\end{itemize}

This curriculum design allows the model to first acquire stable feature representations under easy conditions before being exposed to progressively more challenging weather. 
The balanced sampling explicitly counteracts the natural imbalance between clear and adverse-weather images, ensuring sufficient training on rare but safety-critical scenarios such as heavy rain. 
Overall, this strategy aims to enhance generalization under diverse weather conditions while avoiding the instability or overfitting commonly observed in direct multi-weather training.

\subsection{Metric}
To be same with the related work, we use mean Intersection over Union (mIoU) as the sole evaluation metric for semantic segmentation performance across different weather conditions. The IoU for each class is calculated as the ratio of the intersection area to the union area between the predicted segmentation and the ground truth segmentation. The mIoU is then computed as the average IoU across all classes. formally, for a given class \( c \), the IoU is defined as:

$$IoU_c = \frac{TP_c}{TP_c + FP_c + FN_c}$$

where \( TP_c \) is the number of true positive pixels for class \( c \), \( FP_c \) is the number of false positive pixels, and \( FN_c \) is the number of false negative pixels. The mIoU is then calculated as:

$$mIoU = \frac{1}{N} \sum_{c=1}^{N} IoU_c$$

where \( N \) is the total number of classes. This metric provides a comprehensive measure of segmentation accuracy, accounting for both false positives and false negatives across all classes. A higher mIoU indicates better segmentation performance. The following Figure~\ref{fig:iou} illustrates the visual representation of IoU.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth,height=4cm,keepaspectratio]{pics/iou.png}
    \caption{IoU visual representation (image from the internet).}
    \label{fig:iou}
\end{figure}

\section{Experiments}\label{sec:experiments}
This section presents the experimental setup, results, and analysis of our proposed methodology.

\subsection{Experimental Setup}
All experiments are conducted on a machine with the following specifications:
\begin{itemize}
    \item \textbf{GPU:} NVIDIA GeForce RTX 5080 with 16GB VRAM
    \item \textbf{CPU:} Intel Core Ultra 7 265K @ 5.50GHz
    \item \textbf{RAM:} 32GB DDR4
    \item \textbf{Operating System:} Ubuntu 22.04 LTS
    \item \textbf{Python Version:} Python 3.10
    \item \textbf{Framework:} PyTorch 2.7.1 (for CUDA 12.8)
    \item \textbf{CARLA Version:} 0.9.15
    \item \textbf{Epochs:} 100
\end{itemize}

For more information about the experimental setup, please refer to our GitHub repository (README.md, requirements.txt and other files).

\subsection{Results and Analysis}
As the computational resources are limited, at the time of writing, we finished the experiments of DeepLab V3 under all three training settings. The results are shown in Table~\ref{tab:results}.

\begin{table*}[t]
\centering
% 减小列间距以防止重叠
\setlength{\tabcolsep}{3pt}
\caption{The performance of DeepLab V3 under three training strategies (mIoU). The results are selected from the best epoch during training process.}
\label{tab:results}
\begin{tabular}{ccc}

% --- 第一个表：保留完整索引 ---
\begin{subtable}{0.34\linewidth}
\centering
\caption{Clear-Only (Setting 0)}
\begin{tabular}{lccc}
\toprule
 & \textbf{Noon} & \textbf{Sunset} & \textbf{Night} \\ % 顺序改为 Noon, Sunset, Night
\midrule
\textbf{Clear}     & 0.779 & 0.743 & 0.709 \\
\textbf{SoftRain}  & 0.722 & 0.690 & 0.489 \\
\textbf{HardRain}  & 0.637 & 0.625 & 0.291 \\
\midrule
\textbf{Average}   & \multicolumn{3}{c}{0.6139} \\
\bottomrule
\end{tabular}
\end{subtable}

& % 表格之间的分隔

% --- 第二个表：省略左侧索引列 ---
\begin{subtable}{0.28\linewidth}
\centering
\caption{Mixed-Weather (Setting 1)}
\begin{tabular}{ccc} % 改为3列
\toprule
\textbf{Noon} & \textbf{Sunset} & \textbf{Night} \\ % 顺序改为 Noon, Sunset, Night
\midrule
0.794 & 0.794 & 0.762 \\ 
0.802 & 0.765 & 0.660 \\
0.779 & 0.777 & 0.717 \\
\midrule
\multicolumn{3}{c}{Avg: 0.7759} \\ % 调整 Average 行的写法
\bottomrule
\end{tabular}
\end{subtable}

& % 表格之间的分隔

% --- 第三个表：省略左侧索引列 ---
\begin{subtable}{0.23\linewidth}
\centering
\caption{Curriculum (Setting 2)}
\begin{tabular}{ccc} % 改为3列
\toprule
\textbf{Noon} & \textbf{Sunset} & \textbf{Night} \\ % 顺序改为 Noon, Sunset, Night
\midrule
0.796 & 0.797 & 0.748 \\
0.800 & 0.765 & 0.647 \\
0.778 & 0.776 & 0.707 \\
\midrule
\multicolumn{3}{c}{Avg: 0.7717} \\
\bottomrule
\end{tabular}
\end{subtable}

\end{tabular}
\end{table*}

From the results, we observe some insightful trends. First, in the Clear-Only training setting, the model performs well on easy conditions (e.g., clear weather at noon) as expected, but the performance degrades significantly under adverse weather or nighttime conditions. The mIoU drops from 0.779 in clear noon to 0.291 in hard rain at night, indicating a substantial performance gap. This exhibits the domain transfer challenge caused by weather variations. Moreover, for both Mixed-Weather and Curriculum learning settings, the model shows improved robustness across all weather conditions compared to Clear-Only training. The most difficult scenario is hard rain at sunset, where rain and reflections severely obscure visual features, but the model still achieves reasonable performance above around 0.65 mIoU. The performance gains prove the training strategies we proposed are effective in enhancing model generalization under diverse weather conditions.

The training curves of DeepLab V3 under different training settings are shown in Figure~\ref{fig:training_curves}. Easy to see that both Mixed-Weather and Curriculum learning settings converge to higher mIoU values compared to Clear-Only training, demonstrating the benefits of exposure to diverse weather conditions during training.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.\linewidth,height=6cm,keepaspectratio]{pics/results/Train_curve.png}
    \caption{Training curves of DeepLab V3 under different training settings.}
    \label{fig:training_curves}
\end{figure}

We also visualize the segmentation results of DeepLab V3 under different training settings. Here we show some comparative examples between HardRain at Night in the Figures~\ref{fig:segmentation_results_hardrain_night}.

\begin{figure*}[t]
    \centering
    \begin{subfigure}{.95\linewidth}
        \centering
        \includegraphics[width=1.\linewidth,height=4cm,keepaspectratio]{pics/results/base_0/Hard/vis_hardnight.png}
        \caption{Seetings 0: Clear-Only Training}
    \end{subfigure}

    \begin{subfigure}{.95\linewidth}
        \centering
        \includegraphics[width=1.\linewidth,height=4cm,keepaspectratio]{pics/results/base_1/Hard/vis_hardnight.png}
        \caption{Settings 1: Mixed-Weather Training}
    \end{subfigure}

    \begin{subfigure}{.95\linewidth}
        \centering
        \includegraphics[width=1.\linewidth,height=4cm,keepaspectratio]{pics/results/base_2/Hard/vis_hardnight.png}
        \caption{Settings 2: Weather-Aware Curriculum Learning}
    \end{subfigure}
    \caption{Segmentation results of DeepLab V3 under HardRain at Night condition across different training settings.}
    \label{fig:segmentation_results_hardrain_night}
\end{figure*}

The overall performance seems good under all three training settings, however, upon closer inspection, we observe that Clear-Only training could not accurately segment small and distant objects like pedestrians and signals.

\section{Conclusion}\label{sec:conclusion}
In this project, we investigated the robustness of semantic segmentation models for autonomous driving across diverse weather conditions. We collected a CARLA-simulated dataset with ground truth annotations under various weather and time settings. We evaluated the performance of DeepLab V3 under three different training strategies: Clear-Only Training, Mixed-Weather Training, and Weather-Aware Curriculum Learning. Our experimental results demonstrated that both Mixed-Weather and Curriculum learning strategies significantly improved segmentation performance under adverse weather conditions compared to Clear-Only training. The proposed training strategies effectively enhanced model generalization and robustness, mitigating the domain gap caused by weather variations.

Future work includes extending the evaluation to other semantic segmentation models such as FCN and Lite R-ASPP, and more advanced models like transformer-based architectures. The ablation studies on different curriculum designs and sampling ratios can provide deeper insights into effective training strategies for diverse weather conditions.

\newpage
\section*{Acknowledgements}
We would like to express our sincere gratitude to Professor Hang Qiu, our teaching assistants Shilpa Mukhopadhyay and all of the EE267 course staff for their invaluable support and guidance throughout this project.

Wishing everyone a great winter break and a happy new year!

\bibliography{references}

% \newpage
% \section*{Appendix}


\end{multicols}
\end{document}
